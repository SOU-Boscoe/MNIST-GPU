{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03b9463-d145-4627-a73b-8549bb10d12e",
   "metadata": {},
   "source": [
    "## GPU testing for NAIRR using MNIST -- \n",
    "Introduction to Data Science\n",
    "\n",
    "Professor Bernie Boscoe\n",
    "\n",
    "Southern Oregon University Computer Science Students, Ashland, Oregon USA June 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783035c8-5922-4269-a829-c3d77cb8cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49991efd-898d-49e4-89f2-67991e3c8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how you can install some of these- make sure you add them to path and\n",
    "# some of them might require a kernel restart. Also- good to create an env/ipykernel for these!\n",
    "# Another note-- if you have ~20 students on a NAIRR Ubuntu 60GB Instance with TLJH- you WILL need to mount a volume to put all user files on!\n",
    "#!pip install numpy pandas matplotlib \n",
    "#!pip install torch\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50bd74-3df1-41e4-aaf3-5130ebffd31f",
   "metadata": {},
   "source": [
    "## GPU Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf9bdd-ebab-4343-867a-5714361f6c6c",
   "metadata": {},
   "source": [
    "### Checking that GPU is available in the JuPyTeR notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "092e647d-9400-45e4-8ecf-b2aec275d7f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126\n",
      "CUDA available? True\n",
      "GPU device: GRID A100X-8C\n",
      "Device count:  1\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability and torch installation\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available?\", torch.cuda.is_available())\n",
    "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Device count: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dead55-e76e-41c4-bbd6-1fc22fd16c0b",
   "metadata": {},
   "source": [
    "We need to ensure that the GPU is correctly configured and then we will display the GPU specs below, We will do this for NVIDIA, Intel, and AMD GPU's using `nvidia-smi` and `htop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b75e6b-0699-48ad-a031-1ecbde96e322",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  9 19:27:06 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  GRID A100X-8C                  On  | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   N/A    P0              N/A /  N/A |      0MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\u001b[?1l\u001b>/bin/bash: line 1: kill: $: arguments must be process or job IDs CPU%▽\u001b[30m\u001b[42mMEM%   TIME+  Command      \u001b[11;1H\u001b[30m\u001b[46m  29736 jupyter-be  20   0  8756  4456  3432 R  80.0  0.0  0:00.04 htop         \u001b[12;7H\u001b[m\u001b[m1 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m 20 \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m22\u001b[m\u001b[m984 \u001b[36m14\u001b[m\u001b[m228 \u001b[36m 9\u001b[m\u001b[m492 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.1  0:07.03 /sbin/init\u001b[13;5H372 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m 19 \u001b[31m -1 \u001b[36m 105M 65\u001b[m\u001b[m200 \u001b[36m64\u001b[m\u001b[m176 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.4  0:12.07 /usr/lib/syst\u001b[14;5H419 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m RT \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 282M 27\u001b[m\u001b[m208 \u001b[36m 8\u001b[m\u001b[m648 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.2  0:01.48 /sbin/multipa\u001b[15;5H432 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m 20 \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 282M 27\u001b[m\u001b[m208 \u001b[36m 8\u001b[m\u001b[m648 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.2  0:00.00 \u001b[32m/sbin/multipa\u001b[16;5H\u001b[m\u001b[m433 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m RT \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 282M 27\u001b[m\u001b[m208 \u001b[36m 8\u001b[m\u001b[m648 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.2  0:00.00 \u001b[32m/sbin/multipa\u001b[17;5H\u001b[m\u001b[m434 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m RT \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 282M 27\u001b[m\u001b[m208 \u001b[36m 8\u001b[m\u001b[m648 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.2  0:00.00 \u001b[32m/sbin/multipa\u001b[18;5H\u001b[m\u001b[m435 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m RT \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 282M 27\u001b[m\u001b[m208 \u001b[36m 8\u001b[m\u001b[m648 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.2  0:00.00 \u001b[32m/sbin/multipa\u001b[19;5H\u001b[m\u001b[m436 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m RT \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 282M 27\u001b[m\u001b[m208 \u001b[36m 8\u001b[m\u001b[m648 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.2  0:02.89 \u001b[32m/sbin/multipa\u001b[20;5H\u001b[m\u001b[m439 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m RT \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 282M 27\u001b[m\u001b[m208 \u001b[36m 8\u001b[m\u001b[m648 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.2  0:00.00 \u001b[32m/sbin/multipa\u001b[21;5H\u001b[m\u001b[m441 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m 20 \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m30\u001b[m\u001b[m336 \u001b[36m 8\u001b[m\u001b[m104 \u001b[36m 4\u001b[m\u001b[m904 \u001b[30m\u001b[1mS   0.0 \u001b[m\u001b[m 0.1  0:00.21 /usr/lib/syst\u001b[22;5H799 \u001b[30m\u001b[1mroot       \u001b[m\u001b[m 20 \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m 8\u001b[m\u001b[m484 \u001b[36m 4\u001b[m\u001b[m928 \u001b[36m 1\u001b[m\u001b[m796 \u001b[30m\u001b[1mS   0.0  0.0 \u001b[m\u001b[m 0:01.32 /usr/sbin/hav\u001b[23;5H802 \u001b[35msystemd-oo \u001b[m\u001b[m 20 \u001b[30m\u001b[1m  0 \u001b[m\u001b[36m17\u001b[m\u001b[m424 \u001b[36m 7\u001b[m\u001b[m516 \u001b[36m 6\u001b[m\u001b[m748 \u001b[30m\u001b[1mS   0.0  0.0 \u001b[m\u001b[m 0:02.65 /usr/lib/syst\u001b[24;1HF1\u001b[30m\u001b[46mHelp  \u001b[m\u001b[mF2\u001b[30m\u001b[46mSetup \u001b[m\u001b[mF3\u001b[30m\u001b[46mSearch\u001b[m\u001b[mF4\u001b[30m\u001b[46mFilter\u001b[m\u001b[mF5\u001b[30m\u001b[46mTree  \u001b[m\u001b[mF6\u001b[30m\u001b[46mSortBy\u001b[m\u001b[mF7\u001b[30m\u001b[46mNice -\u001b[m\u001b[mF8\u001b[30m\u001b[46mNice +\u001b[m\u001b[mF9\u001b[30m\u001b[46mKill  \u001b[m\u001b[mF10\u001b[30m\u001b[46mQui\u001b[4ht\u001b[4l\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!htop & sleep 5 ; kill $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9921b25-97ac-4fb3-9a1c-9d55e718bbec",
   "metadata": {},
   "source": [
    "### Ensuring the GPU is availible to use in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9208fd39-9131-4b07-a00c-49b2a2d537d2",
   "metadata": {},
   "source": [
    "Now that we have established that the GPU is detected and running in the Notebook we will check that pytorch is able to utilize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326a13b7-c1c4-41ac-848a-44591a113bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "GRID A100X-8C\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "# setting device on GPU if available, else CPU as a fallback\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae892f6a-dd60-4a85-b666-b80a4b63a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_free_gpu(max_wait_s=300, check_interval_s=10):\n",
    "    \"\"\"Polls every check_interval_s seconds until the GPU can allocate a tiny tensor,\n",
    "       or until max_wait_s elapses.\"\"\"\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        # First, make sure CUDA is even visible\n",
    "        print(\"Checking CUDA availability...\")\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                # Try a minimal allocation\n",
    "                print(\"CUDA available. Attempting minimal allocation...\")\n",
    "                _ = torch.zeros(1, device=\"cuda\")\n",
    "\n",
    "                # If this succeeds, we assume the GPU is free\n",
    "                return\n",
    "            except RuntimeError as e:\n",
    "                print(f\"GPU busy: {e}. Retrying in {check_interval_s}s…\")\n",
    "        else:\n",
    "            print(f\"CUDA not available. Retrying in {check_interval_s}s…\")\n",
    "\n",
    "        if time.time() - start > max_wait_s:\n",
    "            raise RuntimeError(f\"Timed out after {max_wait_s}s waiting for a free GPU\")\n",
    "        time.sleep(check_interval_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82cbe2f0-fda9-4a9f-a63f-97643000b35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU #0: GRID A100X-8C\n",
      "Checking CUDA availability...\n",
      "CUDA available. Attempting minimal allocation...\n",
      "GPU is available.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Count each CUDA device\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    try:\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "    except Exception as e:\n",
    "        name = f\"<error: {e}>\"\n",
    "    print(f\" GPU #{i}:\", name)\n",
    "\n",
    "# Select the device after waiting for it to be free\n",
    "wait_for_free_gpu(max_wait_s=300, check_interval_s=5)\n",
    "print(\"GPU is available.\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3ee4e-c080-41c4-aeeb-7bac1a707da2",
   "metadata": {},
   "source": [
    "GPU is enabled, available and we are now ready to move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9cc1b79-3c1c-4957-89ed-287bf8680b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 60000\n",
      "Testing data: 10000\n"
     ]
    }
   ],
   "source": [
    "# Data loaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                   # convert PIL -> Tensor [0,1]\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # mean & std of MNIST. used to normalize inputs to somewhere between 1 and -1\n",
    "])\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_ds = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "print(f\"Training data: {len(train_ds)}\")\n",
    "print(f\"Testing data: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4acf85",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_features: torch.Size([64, 1, 28, 28])\n",
      "Shape of train_labels: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset loading using Pytorch\n",
    "#mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "#mnist_dataloader = DataLoader(mnist_dataset, batch_size=64, shuffle=True)\n",
    "#train_features,train_labels = next(iter(mnist_dataloader))\n",
    "#print(f\"Shape of train_features: {train_features.shape}\")\n",
    "#print(f\"Shape of train_labels: {train_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48d112f4-6828-478f-a99d-4ebe8be2392b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "# get images\n",
    "imgs = [train_ds[i][0] for i in range(n)]\n",
    "\n",
    "# get corresponding layers\n",
    "labels = [train_ds[i][1] for i in range(n)]\n",
    "\n",
    "# Plot them in a 2×5 grid\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 4))\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    img = imgs[idx].squeeze().numpy()   # shape -> [28,28]\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {labels[idx]}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7ad584-6445-4257-83dd-40a85ec34de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture overview (Note that all these comments were mostly for my (Elyjah Kiene's) own understanding):\n",
    "\n",
    "# Two small conv layers (3×3 kernels, stride 1) reduce the 28×28 input down to 24×24\n",
    "# and find localized features like edges and loops.\n",
    "# note that convolutions are essentially filters. in our model, each kernel has 9\n",
    "# LEARNABLE weights, where each output pixel is the weighted sum of the 9 input\n",
    "# pixels under its 3x3 window. For example:\n",
    "\n",
    "# Let’s say our 3×3 filter (kernel) has the following weights\n",
    "\n",
    "# [ [ w11, w12, w13 ],\n",
    "#   [ w21, w22, w23 ],\n",
    "#   [ w31, w32, w33 ] ]\n",
    "\n",
    "# And our imput image (or previous-layer feature map) is a 5×5 grid of pixel intensities:\n",
    "\n",
    "# [ [ p11, p12, p13, p14, p15 ],\n",
    "#  [ p21, p22, p23, p24, p25 ],\n",
    "#  [ p31, p32, p33, p34, p35 ],\n",
    "#  [ p41, p42, p43, p44, p45 ],\n",
    "#  [ p51, p52, p53, p54, p55 ] ]\n",
    "\n",
    "# To get one output pixel at position (row=2, col=2) in the next feature map, we “center” our 3×3 filter on the 5×5 input’s 3×3 subregion:\n",
    "\n",
    "#     [ p22, p23, p24 ]\n",
    "#     [ p32, p33, p34 ]\n",
    "#     [ p42, p43, p44 ]\n",
    "\n",
    "# Multiply elementwise by the kernel weights, sum them up, and optionally add a bias term.\n",
    "# That sum is the new output pixel at (2,2) in the convolved feature map.\n",
    "\n",
    "# (p22 * w11) + (p23 * w12) + (p24 * w13) ... = p(2,2)\n",
    "\n",
    "# Important note: a \"filter\" (kernel) in convolutions is the 3x3 matrix of learnable weights that \n",
    "# slide over its input to create N \"feature maps\", which are a WxH matrix of individual activations.\n",
    "# when relu is applied, each activation in the feature map is looked at. When a feature map is \"dropped\", \n",
    "# it means each activation in it is zeroed out.\n",
    "\n",
    "# Max pooling (2×2) halves spatial dimensions (24 -> 12), \n",
    "# significantly reducing the feature-map size while still preserving spatial information.\n",
    "\n",
    "# Dropout (0.25 after conv, 0.5 after dense) helps prevent overfitting on a \n",
    "# dataset that’s relatively small (60K train images).\n",
    "\n",
    "# 9216→128 fully connected bottleneck forces the network to compact the \n",
    "# convolutional features into a smaller representation before final classification.\n",
    "\n",
    "# Final log_softmax is exactly what NLLLoss expects, making training stable and straightforward.\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "# We do not need anymore than 10 for this model\n",
    "# due to the effective archetecture\n",
    "num_epochs = 10\n",
    "\n",
    "# load data\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define a simple convolutional neural network\n",
    "# This was retrieved from https://www.geeksforgeeks.org/applying-convolutional-neural-network-on-mnist-dataset/\n",
    "# with a slight tweak from AI (adding the second dropout to prevent overfitting)\n",
    "# and further help converting it from Keras to PyTorch.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "        # learn spatial filters to detect edges, curves, etc.\n",
    "\n",
    "        # 32 filters, results in 32 feature maps. filters here are [3, 3]\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "\n",
    "        # 64 filters, results in 64 feature maps. In this case, each filter is actually\n",
    "        # a 3D block of weight with the shape [32, 3, 3] because it is looking at every one of the 32 previous\n",
    "        # feature maps at the same time in the same relative position.\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        # randomly zeroes out entire channels with probability p during training.\n",
    "        # regularizes the convolutional part to reduce overfitting such that\n",
    "        # the model doesn't rely on a single feature map\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Make first fully connected layer that introduces bottleneck (9212 -> 128)\n",
    "        # This layer learns to combine the 9 216 pooled‐feature activations into 128 higher‐order “concepts.”\n",
    "        # For example, one hidden unit might fire strongly if it sees a loop\n",
    "        # at the top and two vertical strokes beneath (a “9” pattern), \n",
    "        # while another might fire if it sees a slanted line from top‐left\n",
    "        # to bottom‐right plus a little horizontal bridge (a “7” feature pattern).\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        \n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Make second fully connected layer that learns to map the 128 \"concepts\" into 10 raw scores.\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    # Define forward pass\n",
    "    def forward(self, x):\n",
    "        # injects non-linearlity on our first convolution\n",
    "        # creates a shape of [32,26,26]\n",
    "        # note that in normal NNs, the input here would be a 1d 1x784 tensor.\n",
    "        # however, convolutions exploit spacial patterns like loops and edges\n",
    "        # and therefore the actual structure of the image is kept in tact here.\n",
    "        # if we used a normal flattened NN, then points would no longer\n",
    "        # have spacial information, or in other words, the ability to say \n",
    "        # \"this pixel is to the left of this pixel\".\n",
    "        \n",
    "        # Each filter “slides” over the 28×28 input grid,\n",
    "        # taking a weighted sum of each 3×3 subpatch and produces a 26×26 feature map. \n",
    "        # So we end up with 32 feature maps, each 26×26. Tensor shape becomes [batch, 32, 26, 26].\n",
    "        # intuitively each of those 32 filters has (initially random) \n",
    "        # weights that will eventually specialize to “look for” a \n",
    "        # particular 3×3 pattern (say, a vertical edge, or a little curve). \n",
    "        # Wherever that pattern appears in the 28×28 image, that filter’s output map will show a strong activation.\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # apply ReLU to each of the 32 feature maps. Negative sums → zero; positive sums → remain.\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        # second convolution\n",
    "        # creates a shape of [64,24,24]\n",
    "        # Each 3×3 region spans all 32 input channels, \n",
    "        # computing 32×9 multiplications and adding them up to one output pixel. \n",
    "        # Doing that over all 24×24 possible positions yields a 24×24 map. \n",
    "        # And since there are 64 such filters, we end up with [batch, 64, 24, 24].\n",
    "        # now each of those 64 filters “has a view at a 3×3 patch across all 32 feature maps.”\n",
    "        # meaning that each filter is technically of a shape of [32, 3, 3].\n",
    "        # For example, if one of layer-1’s maps highlighted “horizontal edges” \n",
    "        # and another highlighted “vertical edges,” a layer-2 filter might \n",
    "        # learn to fire only when it sees both edge types in one small\n",
    "        # neighborhood meaning its looking at a corner.\n",
    "        x = self.conv2(x)            \n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        # At this point, we can imagine that we have a team of 64\n",
    "        # people who are all really good at finding certain small aspects\n",
    "        # about an image.\n",
    "\n",
    "        # Slides a 2×2 window over each of the 64 24×24 feature-maps. \n",
    "        # For each non-overlapping 2×2 block, it picks the maximum value.\n",
    "        # this downscales ou64 feature maps to 12x12\n",
    "        # [batch,64,12,12]\n",
    "        # since handwriting is sloppy sometimes, this is what gives the \n",
    "        # net some tolerance to slight shifts \n",
    "        # (like if some corner is one pixel to the right, max pooling still catches it).\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "\n",
    "        # dropout a random feature map to prevent too much overfitting on certain feature maps\n",
    "        # A single “channel” after pooling is a 12×12 map that was highlighting, say, horizontal-edge positions. \n",
    "        # If we drop that whole channel, the network can’t rely on “those horizontal edges” \n",
    "        # for this particular input; it must learn to combine other channels \n",
    "        # (maybe diagonal edges, vertical edges) to succeed. Over many training passes, \n",
    "        # the network becomes robust enough that even if any quarter of \n",
    "        # its channels vanish, it still correctly classifies digits.\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # transition from convolutional feature extraction to classification\n",
    "        # collapses dims 1, 2, 3 (64 × 12 × 12) into one long vector of length 9216.\n",
    "        # [batch,64*12*12=9216]\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # first dense layer. Introduces bottleneck from 9216 -> 128\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        # each of those 128 units can be thought of as \n",
    "        # “I see a little loop on top and a diagonal stroke on bottom,” or \n",
    "        # “I see two vertical lines and a horizontal bar.” ReLU zeros out anything negative.\n",
    "\n",
    "        # dropout a random feature map to prevent much overfitting on certain feature maps\n",
    "        # if we have 128 “digit-concept detectors” (loops, corners, slants, etc.), \n",
    "        # then dropping half means the final classification layers must learn to not \n",
    "        # depend on any one concept too heavily. They have to spread out the \n",
    "        # “knowledge” so that even if half of the concepts vanish,\n",
    "        # the net can still decide “this is most likely a 7.”\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # second output dense layer. goes from 128 -> 10 (our desired output classifications)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # produces a probability distribution over the 10 output classes.\n",
    "        # we don't use normal softmax because our loss function (NLLLoss, Negative Log Likelihood Loss)\n",
    "        # expects log-probabilities as input.\n",
    "        # Combining log_softmax + NLLLoss is apparently more numerically \n",
    "        # stable than softmax + CrossEntropyLoss when done separately\n",
    "\n",
    "        # each number 0-9 in this shape corresponds to a log-probability of \n",
    "        # \"how likely is this digit a 0? 3? 8?\"\n",
    "\n",
    "        # Multiplying many small probabilities (which happens in some models) \n",
    "        # can underflow to zero. By summing logs instead, we stay in a safer numerical range.\n",
    "        # softmax here makes each entry sum to 1, so they form a valid probability \n",
    "        # distribution\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# Send our model to the GPU\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# Optimizer and loss function. We could use SDG here as well, but Adam\n",
    "# seems to work a little better in most cases.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We use NLLLoss here in conjunction with log_softmax because\n",
    "# internally, all it does is -log(z) (where z is log(p) in this case due to log_softmax). if we were to use the other common\n",
    "# loss function here, CrossEntropyLoss, internally it does \n",
    "# softmax(z) (turns logits into probabilities) -> log(...) -> nll_loss.\n",
    "# the first softmax call expects a raw logit, not a log probability,\n",
    "# therefore giving us the incorrect loss.\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "224a8534-836a-4737-9f15-1d8e2570a849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training loss: 0.2250\n",
      "Test set accuracy: 98.43%\n",
      "\n",
      "Epoch 2/10, Training loss: 0.0865\n",
      "Test set accuracy: 98.79%\n",
      "\n",
      "Epoch 3/10, Training loss: 0.0674\n",
      "Test set accuracy: 98.85%\n",
      "\n",
      "Epoch 4/10, Training loss: 0.0559\n",
      "Test set accuracy: 98.95%\n",
      "\n",
      "Epoch 5/10, Training loss: 0.0471\n",
      "Test set accuracy: 99.14%\n",
      "\n",
      "Epoch 6/10, Training loss: 0.0426\n",
      "Test set accuracy: 99.20%\n",
      "\n",
      "Epoch 7/10, Training loss: 0.0364\n",
      "Test set accuracy: 99.13%\n",
      "\n",
      "Epoch 8/10, Training loss: 0.0321\n",
      "Test set accuracy: 99.06%\n",
      "\n",
      "Epoch 9/10, Training loss: 0.0305\n",
      "Test set accuracy: 99.11%\n",
      "\n",
      "Epoch 10/10, Training loss: 0.0291\n",
      "Test set accuracy: 99.07%\n",
      "\n",
      "43 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "# for ease, this times this code cell.\n",
    "# This should output something like:\n",
    "# 58.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
    "# where the 58.4 s is how long this code took to run\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # enter training mode. This tells pytorch that parameters are going to be changed.\n",
    "    # for example, our dropout layers will randomly zero out activations during training, \n",
    "    # but this should be turned off that randomness during evaluation (happens when we enter evaluation mode).\n",
    "    # We want dropout active when we're updating weights, \n",
    "    # because that regularizes the model. But we don’t want dropout \n",
    "    # “dropping” units while we're simply measuring how well the model performs at the end of an epoch.\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        # move 128 image batch to the GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Every time we call .backward() (later in this loop), \n",
    "        # PyTorch accumulates gradients into each parameter’s .grad field.\n",
    "        # We call zero_grad() to reset all gradients to zero before computing \n",
    "        # new ones for this batch. Otherwise, gradients from the previous batch would mix in.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Runs the forward pass on the 128 batch\n",
    "        output = model(data)\n",
    "\n",
    "        # criterion was defined as nn.NLLLoss(). That takes the model’s\n",
    "        # log-probabilities and the true labels (integers 0–9) to \n",
    "        # produce a single scalar: the AVERAGE “negative log likelihood” over this batch of 128.\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # The magical backpropagation step that PyTorch handles for us.\n",
    "        # After this, each param.grad field is populated with the \n",
    "        # gradient of the loss with respect to that parameter, evaluated on this minibatch.\n",
    "        # this essentially is what points us to the minima of our error surface. \n",
    "        # This does not update our weights and biases, it only tells us which direction to \n",
    "        # step and with what magnitude to reduce loss.\n",
    "        loss.backward()\n",
    "\n",
    "        # This is our gradient descent step, Adam (similar to SGD, or stochastic gradient descent).\n",
    "        # This is where the weights and baises are nudged such that the next time the\n",
    "        # same inputs appear, the predicted probabilities should (hopefully) be closer to the correct labels.\n",
    "        # this uses the gradients calculated from backpropagation and actually applies them and updates\n",
    "        # our weights and biases.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add to total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Get our average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Validation step for this epoch to track progress\n",
    "    # Switch to evaluation mode.\n",
    "    # In our case, the dropout layers will now become inactive.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Send test batch to GPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # argmax(dim=1) picks, for each image in the batch, \n",
    "            # “which of the 10 classes has the largest log-probability?”. \n",
    "            # A 1D tensor of length batch_size, e.g. [3, 7, 1, 1, 9, …], one predicted digit per image.\n",
    "            pred = model(data).argmax(dim=1)\n",
    "\n",
    "            # Compare each predicted digit to the true label\n",
    "            correct += (pred == target).sum().item()\n",
    "\n",
    "    # Get our accuracy\n",
    "    accuracy = correct / len(test_ds)\n",
    "    print(f\"Test set accuracy: {accuracy*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9222a52-340f-4aae-8fda-cdd3b9bd54e0",
   "metadata": {},
   "source": [
    "## Why, in summary, this architecture works with such little epochs\n",
    "\n",
    "Locality: Digit classification depends on tiny local strokes (edges, curves).\n",
    "\n",
    "- 3×3 convolutions are perfect for capturing these, setting the stage for accurate classification immediately.\n",
    "\n",
    "Hierarchical Composition: Next layer builds bigger patterns out of smaller ones.\n",
    "\n",
    "- Two conv layers can already see “loops”, “crossings”, “endpoints” etc.\n",
    "\n",
    "Spatial Invariance: Max‐pool makes the network robust to small shifts or jitters.\n",
    "\n",
    "- Handwriting is messy but pooling gives a little slack.\n",
    "\n",
    "Regularization: Dropout (25 % on conv feature maps, 50 % on dense layer) forces the model to generalize.\n",
    "\n",
    "- Prevents overfitting on a relatively small dataset.\n",
    "\n",
    "In other words, the network looks for little strokes and puts them together into more recognizable shapes. Then pools to ignore tiny shifts and compacts everything into a handful of important ‘digit‐concept’ signals. Finally decides which digit fits best. Its size and dropout mean it’s big enough to learn all ten digits but small enough to not memorize every quirk.\n",
    "\n",
    "The effectiveness here lies in the convoulations we used, in our case a convolutional kernel (e.g. 3×3) only sees a 3×3 patch of the image at a time. This is fine, because edges, small corners, and curves are inherently localized features—if we stare at just a 3×3 or 5×5 block of pixels, we can already tell if there’s a dark‐to‐light transition or a corner/curve forming.\n",
    "\n",
    "By focusing on small areas, each convolutional filter can become a “specialist” at recognizing one simple pattern (vertical edge, horizontal edge, diagonal line, small curve, etc.) wherever it appears in the image, making it effective for finding handwriting patterns.\n",
    "\n",
    "---\n",
    "\n",
    "Calculating the number of parameters in this model (for fun)\n",
    "\n",
    "Conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "\n",
    "- Each of the 32 filters is shape [in_channels=1, height=3, width=3], or 9 weights per filter.\n",
    "\n",
    "- Total weights in conv1 = 32 × 1 × 3 × 3 = 288\n",
    "\n",
    "- Plus 32 bias terms (one per filter) = 32.\n",
    "\n",
    "So conv1 alone contributes 288 + 32 = 320 scalar parameters.\n",
    "\n",
    "- Conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "\n",
    "- Now each of the 64 filters is shape [in_channels=32, height=3, width=3], or 288 weights per filter.\n",
    "\n",
    "- Total conv2 weights = 64× 32 × 3 × 3 = 64 × 288 = 18,432\n",
    "\n",
    "- Plus 64 biases = 64.\n",
    "\n",
    "- So conv2 contributes 18,432 + 64 = 18,4966 parameters.\n",
    "\n",
    "After those conv blocks, we flattened into 9,216 features and fed into a 128-unit linear layer:\n",
    "\n",
    "- fc1 = nn.Linear(9216, 128)\n",
    "\n",
    "- That has 9216 × 128 = 1,179,648 weights, plus 128 biases = 1,179,776 parameters.\n",
    "\n",
    "- fc2 = nn.Linear(128, 10)\n",
    "\n",
    "- That has 128 × 10 = 1,280 weights, plus 10 biases = 1,290 parameters.\n",
    "\n",
    "**320  +  18,496  +  1,179,776  +  1,290  =  1,199,882 total parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120f256-6175-4f06-9fd6-c811b959aac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f191756-c768-415e-b216-019b6d028452",
   "metadata": {},
   "source": [
    "## We now show some different architectures to run MNIST, and how long it takes, showing the need for GPUs when training models with big data, for example images, and for many epochs. For example, MNIST images are greyscale and 28 x 28 pixels, so any larger images will require more compute power!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1b231-a625-45e2-a3b1-8fd28d1b2d6a",
   "metadata": {},
   "source": [
    "## Example 1: Running MNIST on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302938b-ae8f-46f3-a663-1582961b4c97",
   "metadata": {},
   "source": [
    "By: Jackson Harrower and Isabella Cooper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a224082-923d-4acc-be74-3eb05c56b38f",
   "metadata": {},
   "source": [
    "### Jackson's Run\n",
    "My code loaded the MNIST dataset on kaggle notebook editor used strictly CPU through tensorflow and for 50 epochs took 310.79 seconds or 5 minutes 10 seconds, the final epoch had accuracy: 0.9989 and loss: 0.0033"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1e7c5",
   "metadata": {},
   "source": [
    "### Performance Notes\n",
    "- Trained the model for 50 epochs\n",
    "- First MNIST dataset load (from file) took **~ 349 seconds** using CPU\n",
    "- Second dataset load took **~351.55 seconds**\n",
    "- Model training was completed without GPU acceleration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5139e41f-b1b1-481f-8a33-4221b657ef04",
   "metadata": {},
   "source": [
    "## Example 2: Bonus: Running under Apple Metal\n",
    "\n",
    "By: Brent Monning\n",
    "\n",
    "Of course, CUDA is not the only parallel computing technology that can be used to train a model off MNIST, given that it is designed to only work with Nvidia cards. One can use ROCM, OpenCL, DirectML, Metal, and whatnot, but this short bonus section will be dedicated to how model training performs under Metal. The model training will be done in Tensorflow, which will determine optimal hardware as soon as the module is loaded, and testing was done on an Apple M1 with 7 GPU cores.\n",
    "\n",
    "One thing that is noticable immediately upon training without an accuracy cap is that, despite the same snippet, the Metal driver will just consistently generate more and more error. Around (put elapsed here) went by.\n",
    "\n",
    "![Loss and Accuracy Graphs of Uncapped M1](img/metal-uncapped.png)\n",
    "\n",
    "On the other hand, the CUDA notebook with the same snippet does not have this issue. This took (put elapsed here).\n",
    "\n",
    "![Loss and Accuracy Graphs of Uncapped CUDA Instance](img/cuda-uncapped.png)\n",
    "\n",
    "When given a cap of 98.5% accuracy, where training will immediately conclude if this accuracy was exceeeded, the M1 takes (put elapsed here), whilst the CUDA instance on another machine takes (put elapsed here). The accuracy is about the same as shown below, albeit there are far less training epochs that were performed as opposed to the 50 training epochs prior. [There is a GitHub issue open on the Tensorflow Keras source repository, and whilst it is closed and said to be resolved in newer versions, some say it is still a problem.](https://github.com/keras-team/tf-keras/issues/140)\n",
    "\n",
    "Below is the M1:\n",
    "\n",
    "![Loss and Accuracy Graphs of Capped M1](img/metal-capped.png)\n",
    "\n",
    "And below is the CUDA instance:\n",
    "\n",
    "![Loss and Accuracy Graphs of Capped CUDA Instance](img/cuda-capped.png)\n",
    "\n",
    "For the next few, we'll only be focusing on the Metal results using the M1 chip. Two confusion matrixes, one using a linear colorbar for values, and another using a logarithmic colorbar, are provided below:\n",
    "\n",
    "![Confusion Matrix using a Linear Colorbar](img/metal-confusion-linear.png)\n",
    "\n",
    "![Confusion Matrix using a Logarithmic Colorbar](img/metal-confusion-logarithmic.png)\n",
    "\n",
    "And here is a scatterplot visualization of each numeral using Scikit Learn's TSNE module.\n",
    "\n",
    "![TSNE Cluster Scatterplot visualization of our Metal-powered model](img/metal-tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef931c-67b1-4441-8b79-dbdad1685588",
   "metadata": {},
   "source": [
    "# Example 3: Keras Neural Network \n",
    "By: Jeovanni Mendez, Laura Lovrein\n",
    "<p style=\"font-size:25px;\">When we used the  MNIST dataset to train a neural network model using Keras via Tensorflow, training with 50 epochs using CPU only took a total of 386.25 seconds with an accuracy of 0.979.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f9541-3e4f-45a8-820f-7ccce9061759",
   "metadata": {},
   "source": [
    "# Example 4: MNIST with google colab using tensorflow\n",
    "by: Bailey Baker, Julian Loutzenhiser, and Katherine Nunn\n",
    "\n",
    "<p style=\"font-size:25px;\">Using tensorflow in a google colab notebook, the model took an average of 49.3 seconds to train with 10 epochs and achieved a test accuracy of 97.97% with 0.2971 loss. <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0ff0d",
   "metadata": {},
   "source": [
    "# Example 5: MNIST with Keras\n",
    "by: Nate Cook\n",
    "\n",
    "<p style=\"font-size:25px;\"> I used Keras with 50 epochs on the MNIST dataset, training took 996.63 seconds and I ended with a accuracy of .993."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
